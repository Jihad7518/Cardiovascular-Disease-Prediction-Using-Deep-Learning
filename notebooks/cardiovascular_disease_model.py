# -*- coding: utf-8 -*-
"""Cardiovascular Disease_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ovvesl-6yYKryXJ185bzDNtQQhjdpEob

<h1>CSE475 Machine Learning Project </h1>
<h2>Identifying  Cardiovascular Disease Using a cutting-edge Deep Learning Technique </h2>
"""

#importing libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

#!pip install keras

#!pip install tensorflow
import keras
from keras.layers import Dense, BatchNormalization, Dropout, LSTM
from keras.models import Sequential
from keras import callbacks
import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv('heart_data.csv')
df.head()

df.drop(['index', 'id'], axis = 1, inplace = True)
df['age']=(df['age']/365).astype(int)
df

df['bmi'] = df['weight'] / ((df['height'] / 100) ** 2)

df.describe().T

df_out=df[df['bmi'].between(25, 40)]
df_out=df_out[df_out['weight'].between(40, 120)]
df_out=df_out[df_out['height'].between(130, 180)]
df_out=df_out[df_out['ap_hi'].between(90, 150)]
df_out=df_out[df_out['ap_lo'].between(70, 110)]
df_out=df_out[df_out['age'].between(39, 65)]
df_out

df_out.describe().T

X = df_out.drop('cardio', axis=1)
y = df_out['cardio']

colors =["#CD5C5C","#F08080","#FA8072","#E9967A","#FFA07A"]
plt.figure(figsize=(20,10))
sns.boxenplot(data = X,palette = colors)
plt.xticks(rotation=60)
plt.show()

# Setting up a standard scaler for the features and analyzing it thereafter

from sklearn import preprocessing
col_names = list(X.columns)
s_scaler = preprocessing.StandardScaler()
X_scaled= s_scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=col_names)
X_scaled.describe().T

colors =["#CD5C5C","#F08080","#FA8072","#E9967A","#FFA07A"]
plt.figure(figsize=(20,10))
sns.boxenplot(data = X_scaled,palette = colors)
plt.xticks(rotation=60)
plt.show()

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.layers import Dropout
from keras import regularizers

early_stopping = callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True)

def create_model():
    # Initialising the NN
    model = Sequential()
    # layers
    model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))
    model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))
    model.add(Dropout(0.25))
    model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))
    model.add(Dropout(0.5))
    model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))
    # Compiling the ANN
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
    return model

print(model.summary())

import numpy as np
import tensorflow as tf
from sklearn.model_selection import KFold
from sklearn.metrics import accuracy_score

# Define the number of folds for cross-validation
num_folds = 10
kf = KFold(n_splits=num_folds)

# Perform K-fold cross-validation
fold_accuracy = []

i=1
for train_index, test_index in kf.split(X_scaled,y):
   # Print out the train and test indices for each fold
    print("Train Index:", train_index, train_index.size)
    print("Test Index:", test_index, train_index.size)
    X_train, X_test = X_scaled.iloc[train_index], X_scaled.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Create the model
    model = create_model()

    # Train the model
    model.fit(X_train, y_train, epochs=10, batch_size=25)

    # Evaluate the model on the test data
    accuracy = model.evaluate(X_test, y_test, verbose=0)
    print()
    print(f"For {i} fold training: Loss - {accuracy[0]} accuracy - {accuracy[1]}")
    print()
    i=i+1
    fold_accuracy.append(accuracy[1])

# Calculate and print the average accuracy across all folds
average_accuracy = np.mean(fold_accuracy[0])
print("Average accuracy:", average_accuracy)

y_pred = model.predict(X_test)

import seaborn as sns
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm,annot=True,cmap="Blues",fmt="d",cbar=False)
#accuracy score
from sklearn.metrics import accuracy_score
ac=accuracy_score(y_test, y_pred.round())
print('accuracy of the model: ',ac)







"""<H2> Using encoding"""



#,'height','weight'
df_encoded = pd.get_dummies(df_out, columns=['gender', 'cholesterol', 'gluc','smoke',	'alco'	,'active'])
#df_encoded.drop(['gender_2', 'cholesterol_3','gluc_3','smoke_1','alco_1','active_1'], axis = 1, inplace = True)
df_encoded

X = df_encoded.drop('cardio', axis=1)
y = df_encoded['cardio']

X_train, X_test, y_train,y_test = train_test_split(X_scaled,y,test_size=0.3,random_state=25)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Train the ANN
history = model.fit(X_train, y_train, batch_size = 25, epochs = 10)

y_pred = model.predict(X_test)

import seaborn as sns
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm,annot=True,cmap="Blues",fmt="d",cbar=False)
#accuracy score
from sklearn.metrics import accuracy_score
ac=accuracy_score(y_test, y_pred.round())
print('accuracy of the model: ',ac)











"""<h2>3rd Model binary Classification"""

df_out

X = np.array(df_out.drop('cardio', axis=1))
y = np.array(df_out['cardio'])

mean = X.mean(axis=0)
X -= mean
std = X.std(axis=0)
X /= std

# create X and Y datasets for training
from sklearn import model_selection

X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=42, test_size = 0.2)

Y_train_binary = y_train.copy()
Y_test_binary = y_test.copy()

Y_train_binary[Y_train_binary > 0] = 1
Y_test_binary[Y_test_binary > 0] = 1

print(Y_train_binary[:20])

model = Sequential()
model.add(Dense(16, input_dim=12, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(8, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))
model.add(Dropout(0.25))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

print(model.summary())

history=model.fit(X_train, Y_train_binary, validation_data=(X_test, Y_test_binary), epochs=50, batch_size=10)

y_pred = model.predict(X_test)

import seaborn as sns
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred.round())
sns.heatmap(cm,annot=True,cmap="Blues",fmt="d",cbar=False)
#accuracy score
from sklearn.metrics import accuracy_score
ac=accuracy_score(y_test, y_pred.round())
print('accuracy of the model: ',ac)